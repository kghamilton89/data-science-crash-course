{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of Queries\n",
    "\n",
    "One of the most common things that you'll be asked to do as a data scientist (especially as a data scientist who is working anywhere near text) is answer questions like:\n",
    "- \"What is the most common search query, or text input, that this product receives?\"\n",
    "- \"What kind of metadata characteristics do these queries have?\" (this is just a bad way of generalizing the kinds of questions that are going to be specific to your problem domain-- more on this later)\n",
    "\n",
    "This section of the Data Science Crash Course is going to be focused on making general observations about text-based fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first few rows of dataframe:\n",
      "                                     id  \\\n",
      "0  b668ae91-bbdc-491a-bca8-b4a8e4a07d0b   \n",
      "1  5260c1a0-39ca-433f-9ce4-114d67bcc4ff   \n",
      "2  ad9f9d9f-f511-49f5-afae-fa98c0b2c78b   \n",
      "3  80218e6a-dcbd-4d76-a6be-4c36d09454ef   \n",
      "4  b6ada662-6599-4b60-ba8c-0b944234ca01   \n",
      "\n",
      "                                               query  \\\n",
      "0                    Зеленый угодный изучить металл.   \n",
      "1                      Выражаться кузнец коричневый.   \n",
      "2  Вывести неправда изредка избегать поколение во...   \n",
      "3               Нож через видимо выгнать советовать.   \n",
      "4  Аллея призыв космос за монета появление совето...   \n",
      "\n",
      "                                            response        time  feedback  \\\n",
      "0          Забирать рай пламя. Отдел магазин металл.  1723578498  negative   \n",
      "1  Точно миг правый необычный тута. Порядок рабоч...  1740303596  positive   \n",
      "2  Белье спешить другой запустить. Расстегнуть го...  1734326957  positive   \n",
      "3  Палка болото плавно подробность постоянный вск...  1739501025  positive   \n",
      "4  Засунуть беспомощный стакан решение желание вт...  1736250471  positive   \n",
      "\n",
      "  platform  registered  pro_user  \n",
      "0  desktop       False     False  \n",
      "1  desktop        True     False  \n",
      "2   mobile        True     False  \n",
      "3   mobile        True      True  \n",
      "4   mobile        True     False  \n",
      "------\n",
      "number of rows in dataframe\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# first, let's import all the libraries we'll need for this exercise\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# as always, we'll need to define the data we'd like to use for this exercise\n",
    "# i had fun using the pickled russian-language llm query data, let's use it again\n",
    "file_path = '../data/llm_usage_ru_dataset.pkl'\n",
    "\n",
    "# we'll need to unpickle our list of dictionaries before declaring our dataframe\n",
    "with open(file_path, \"rb\") as f:\n",
    "    file = pickle.load(f)\n",
    "\n",
    "# declare our dataframe\n",
    "df = pd.DataFrame(file)\n",
    "\n",
    "# have a quick look at the data we've just loaded\n",
    "print('first few rows of dataframe:')\n",
    "print(df.head())\n",
    "print('------')\n",
    "print('number of rows in dataframe')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning our dataset\n",
    "\n",
    "When we use a text-based dataset, there are a few steps we need to take before we start analyzing our data. Here are a couple of the most standard steps that you'll see data scientists and analysts take before working with text:\n",
    "- Convert all text to lower case: this let's us more easily count similar queries because a machine will understand \"this\" and \"This\" as different queries when really, they are the same!\n",
    "- \"Strip\" leading and trailing whitespace: this may just be a case of user error or some kind of tokenization process where certain queries will have \" extra spaces\" before or \"extra spaces \" after the core search term.\n",
    "- Remove stopwords: stopwords are very common words which might muddle the results of a text count, more on this in just a moment.\n",
    "\n",
    "Let's work through the cleaning process and let have a look at our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our stopwords\n",
    "ru_stopwords = ['а', 'и', 'в', 'на', 'не', 'с', 'о', 'к', 'за', 'по', \n",
    "'из', 'до', 'как', 'что', 'где', 'когда', 'кто', 'какой', \n",
    "'который', 'этот', 'тот', 'свой', 'весь', 'каждый', \n",
    "'один', 'два', 'три', 'четыре', 'пять', 'шесть', \n",
    "'семь', 'восемь', 'девять', 'десять', 'первый', \n",
    "'второй', 'третий', 'четвёртый', 'пятый', 'мой', \n",
    "'твой', 'наш', 'ваш', 'его', 'её', 'их', 'был', \n",
    "'была', 'было', 'были', 'есть', 'нет', 'да', 'но', \n",
    "'или', 'если', 'чтобы', 'потому', 'поэтому', 'тоже', \n",
    "'также', 'даже', 'только', 'уже', 'ещё', 'вот', \n",
    "'там', 'здесь', 'туда', 'сюда', 'оттуда', 'откуда', \n",
    "'всегда', 'никогда', 'сейчас', 'тогда', 'раньше', \n",
    "'позже', 'быстро', 'медленно', 'хорошо', 'плохо', \n",
    "'можно', 'нужно', 'должен', 'может'\n",
    "]\n",
    "\n",
    "# convert all of our queries to lower case and strip leading and trailing whitespace\n",
    "df['clean_query'] = df['query'].str.lower().str.strip()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yandex-tt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
