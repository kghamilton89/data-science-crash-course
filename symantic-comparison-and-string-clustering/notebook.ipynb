{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symantic Comparison and String Clustering\n",
    "\n",
    "So remember, at the start of this whole exercise we learned that we are looking at a series of search queries-- probably made against a search engine or something similar.\n",
    "\n",
    "In this notebook, we're going to walk through a few techniques to compare those strings so that we can get a better understanding of which of these are similar and for which reasons.\n",
    "\n",
    "Symantic comparison and string matching can be a little tricky to understand, so I'll make sure to try to explain all of the concepts thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_query</th>\n",
       "      <th>time</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vacation spots recipe</td>\n",
       "      <td>1722760240</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is invest in crypto</td>\n",
       "      <td>1734252942</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI tools recipe</td>\n",
       "      <td>1728010297</td>\n",
       "      <td>desktop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>best camera symptoms</td>\n",
       "      <td>1730697978</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best meditation near me</td>\n",
       "      <td>1735175397</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               search_query        time platform\n",
       "0     vacation spots recipe  1722760240   mobile\n",
       "1  what is invest in crypto  1734252942   mobile\n",
       "2           AI tools recipe  1728010297  desktop\n",
       "3      best camera symptoms  1730697978   mobile\n",
       "4   best meditation near me  1735175397   mobile"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If you need help importing data or understanding what is happening in this cell, check out this notebook: ~/uploading-and-inspecting-data/notebook.ipynb\n",
    "\n",
    "file_path = '../data/data.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having a closer look\n",
    "\n",
    "Now that we've created our DataFrame, we need to normalize all of this text data so that we can more readily make some comparisons between rows. Extra characters, strange (or even typical) capitalization of characters, whitespace, and tons of other things that might seem innocuous to a human reader, can actually have a really large impact on the ability of machines to compare two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       vacation spots recipe\n",
       "1    what is invest in crypto\n",
       "2             ai tools recipe\n",
       "3        best camera symptoms\n",
       "4     best meditation near me\n",
       "Name: search_query, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first thing we'll do is convert all of our queries to lower case\n",
    "queries = df['search_query'].str.lower()\n",
    "\n",
    "# The next thing we'll do is remove any leading or trailing whitespace\n",
    "queries = queries.str.strip()\n",
    "\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on...\n",
    "\n",
    "We can see above that we've changed words like \"AI\" to \"ai\". By ensuring that our data is normalized, we'll make it a lot easier to compare strings that may be symatically very similar, but structurally slightly different. \n",
    "\n",
    "Your dataset may already be normalized, in which case, you might be able to skip this step (unless you want to look through you entire, potentially quite large, dataset to be sure though, I always recommend running the above cell just in case. It takes way less time that it would if you wanted to look through the entire dataset!)\n",
    "\n",
    "Okay, now that we've normalized all of our strings, we need to convert them to a language that machines can understand, and what better language than **numbers** for that purpose?!\n",
    "\n",
    "For now, it's enough to know that `TfidfVectorizer` is a function that converts you human-readable string into a number-based matrix that can be used for comparison. What's happening under the hood is actually quite interesting as well though, so make sure you read the [`sklearn` documentation](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py) to learn more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word about stop words\n",
    "\n",
    "Before we move on with the comparison, let's talk about stop words.\n",
    "\n",
    "Stop words are common words in the English language that should also be removed during text-based comparisons. Things like `a` or `and` or `or` can also skew the results of comparison. For example the string `lions and tigers and bears` is far less symatically similar to the string `happy and joyful and excited` if we remove the stop word `and` which doesn't actually have much relevant meaning for our purposes here.\n",
    "\n",
    "That said, this is an English-language tutorial, and we all know that data science is practiced in nearly every language on the globe. If you're using `sklearn` whilst anaylzing data in another language (or even, if you have a domain-specific dataset that contains certain words which occur over and over again but don't deliver any symantic value to the underlying data), add a custom dictionary of stop words is a great way to improve your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost']\n",
      "Total stop words:\n",
      "318\n"
     ]
    }
   ],
   "source": [
    "# First, let's have a look at the stopwords contained in the default `english` parameter\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# We only show the first 10 because the amount of stopwords is quite large\n",
    "print(sorted(list(ENGLISH_STOP_WORDS))[:10])\n",
    "print('Total stop words:')\n",
    "print(len(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom stopword dictionary\n",
    "\n",
    "Let's consider a case where we're going to be working with a Russian-language dataset. If we remove English stopwords alone, we're going to run into the same matching issues discussed above, but this time for the Russian language.\n",
    "\n",
    "This part of the tutorial is going to focus on creating a custom stopword dictionary and using it instead of the default `english` dictionary.\n",
    "\n",
    "(If you'd just like to update the default `english` language stop words, have a look at this [Stack Overflow issue](https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list) which describes doing just that!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define our custom stopwords.\n",
    "custom_stop_words = [\n",
    "    'в', 'на', 'по', 'о', 'об', 'про', 'при', 'за', 'под', 'над', 'с', 'без', 'до', 'после', 'из', 'от', 'к', 'у', 'через',\n",
    "    'и', 'но', 'или', 'а', 'да', 'потому что', 'так как', 'если', 'когда', 'чтобы',\n",
    "    'не', 'ни', 'же', 'ведь', 'уж', 'бы', 'ли', 'разве', 'неужели',\n",
    "    'я', 'ты', 'он', 'она', 'оно', 'мы', 'вы', 'они', 'свой', 'наш', 'ваш',\n",
    "    'весь', 'сам', 'самый', 'каждый', 'любой', 'оба', 'тоже', 'также',\n",
    "    'это', 'там', 'тут', 'тогда', 'сейчас', 'всегда', 'никогда', 'часто',\n",
    "    'конечно', 'может', 'должен', 'можно', 'нужно',\n",
    "    'вся', 'сама', 'самая', 'каждая', 'любая', 'обе', 'эта',\n",
    "    'там', 'тут', 'тогда', 'сейчас', 'всегда', 'никогда', 'часто',\n",
    "    'конечно', 'может', 'должна', 'можно', 'нужно'\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yandex-tt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
